# Инструкция по подключению моделей LLM

Система поддерживает два типа провайдеров моделей:
- **OpenAI** (облачные модели GPT) - требует интернет и API ключ
- **Ollama** (локальные модели) - работает без интернета на вашем компьютере

## Быстрый старт

### Вариант 1: OpenAI (Облачные модели)

1. Получите API ключ на https://platform.openai.com/api-keys
2. Добавьте в файл `.env`:
```bash
LLM_PROVIDER_TYPE=openai
LLM_MODEL=gpt-4o
OPENAI_API_KEY=ваш_ключ_здесь
```

3. Перезапустите приложение

### Вариант 2: Ollama (Локальные модели)

1. Установите Ollama:
   - macOS: `brew install ollama` или скачайте с https://ollama.ai
   - Linux: `curl -fsSL https://ollama.ai/install.sh | sh`
   - Windows: скачайте установщик с https://ollama.ai

2. Запустите Ollama:
```bash
ollama serve
```

3. Скачайте модель (например, llama3.2):
```bash
ollama pull llama3.2
```

Доступные модели:
- `llama3.2` - быстрая и эффективная (рекомендуется)
- `llama3.1` - более мощная версия
- `mistral` - альтернативная модель
- `qwen2.5` - хорошая поддержка русского языка
- `phi3` - компактная модель

4. Добавьте в файл `.env`:
```bash
LLM_PROVIDER_TYPE=ollama
LLM_MODEL=llama3.2
OLLAMA_BASE_URL=http://localhost:11434
```

5. Перезапустите приложение

## Настройка через веб-интерфейс

1. Откройте админ панель: http://localhost:5003/admin
2. Перейдите в раздел "Настройки моделей LLM"
3. Выберите тип провайдера (OpenAI или Ollama)
4. Выберите модель из списка доступных
5. Для Ollama укажите URL сервера (по умолчанию http://localhost:11434)
6. Нажмите "Тестировать подключение" для проверки
7. Нажмите "Сохранить настройки"

## Переключение между провайдерами

Вы можете переключаться между OpenAI и Ollama в любой момент через веб-интерфейс или изменив переменные окружения.

**Важно:** При переключении настройки применяются сразу, но для постоянного сохранения рекомендуется обновить файл `.env`.

## Рекомендации по выбору модели

### OpenAI (Облачные)
- **gpt-4o** - лучшая модель для юридических задач (рекомендуется)
- **gpt-4-turbo** - хороший баланс качества и скорости
- **gpt-3.5-turbo** - быстрая и дешевая, но менее точная

**Плюсы:** Высокое качество, быстрая работа, не требует ресурсов компьютера
**Минусы:** Требует интернет, платно, данные отправляются в облако

### Ollama (Локальные)
- **llama3.2** - хороший баланс качества и скорости (рекомендуется)
- **qwen2.5** - отличная поддержка русского языка
- **mistral** - быстрая и эффективная

**Плюсы:** Бесплатно, работает без интернета, данные остаются локально
**Минусы:** Требует мощный компьютер (минимум 8GB RAM), медленнее облачных моделей

## Требования к системе для Ollama

- **Минимум:** 8GB RAM, 4 CPU cores
- **Рекомендуется:** 16GB+ RAM, 8+ CPU cores
- **Для больших моделей:** 32GB+ RAM, GPU (опционально)

## Устранение проблем

### OpenAI не работает
- Проверьте, что API ключ правильный
- Убедитесь, что на счету есть средства
- Проверьте интернет-соединение

### Ollama не работает
- Убедитесь, что Ollama запущена: `ollama serve`
- Проверьте, что модель скачана: `ollama list`
- Проверьте URL в настройках (по умолчанию http://localhost:11434)
- Проверьте логи Ollama на наличие ошибок

### Модель не отвечает
- Для Ollama: проверьте, что модель скачана и доступна
- Для OpenAI: проверьте лимиты API и баланс
- Попробуйте другую модель
- Проверьте логи приложения

## Дополнительные настройки

Все настройки можно изменить в файле `.env`:

```bash
# Тип провайдера
LLM_PROVIDER_TYPE=ollama

# Модель
LLM_MODEL=llama3.2

# URL Ollama (только для Ollama)
OLLAMA_BASE_URL=http://localhost:11434

# OpenAI ключ (только для OpenAI)
OPENAI_API_KEY=sk-...
```

## API для управления моделями

Система предоставляет REST API для управления моделями:

- `GET /api/settings/llm` - получить текущие настройки
- `POST /api/settings/llm` - обновить настройки
- `POST /api/settings/llm/test` - протестировать подключение

Пример использования:
```bash
# Получить настройки
curl http://localhost:5003/api/settings/llm

# Обновить настройки
curl -X POST http://localhost:5003/api/settings/llm \
  -H "Content-Type: application/json" \
  -d '{"provider_type": "ollama", "model": "llama3.2"}'
```

