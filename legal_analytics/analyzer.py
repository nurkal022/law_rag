"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∫ –∑–∞–∫–æ–Ω–æ–ø—Ä–æ–µ–∫—Ç–∞–º
"""

import re
from typing import List, Dict, Any, Tuple
from collections import Counter, defaultdict
import random
import math
import json
from datetime import datetime, timedelta
import numpy as np
from textblob import TextBlob
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import SnowballStemmer


class LegalCommentAnalyzer:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∫ –∑–∞–∫–æ–Ω–æ–ø—Ä–æ–µ–∫—Ç–∞–º"""
    
    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLTK –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('punkt_tab', quiet=True)
            nltk.download('stopwords', quiet=True)
            nltk.download('averaged_perceptron_tagger', quiet=True)
        except:
            pass
        
        self.stemmer = SnowballStemmer('russian')
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        self.positive_words = [
            '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é', '—Å–æ–≥–ª–∞—Å–µ–Ω', '—Ö–æ—Ä–æ—à–æ', '–æ—Ç–ª–∏—á–Ω–æ', '–≤–∞–∂–Ω–æ', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ',
            '–ø–æ–ª–µ–∑–Ω–æ', '—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ', '—Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ', '–ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ', '–∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ',
            '–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ', '–±–ª–∞–≥–æ–ø—Ä–∏—è—Ç–Ω–æ', '—É–ª—É—á—à–µ–Ω–∏–µ', '—Ä–∞–∑–≤–∏—Ç–∏–µ', '–ø–æ–¥–¥–µ—Ä–∂–∫–∞',
            '–æ–¥–æ–±—Ä—è—é', '—Ä–µ–∫–æ–º–µ–Ω–¥—É—é', '—Ü–µ–Ω–Ω–æ', '–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ', '—É—Å–ø–µ—à–Ω–æ', '–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ',
            '–≤—ã–≥–æ–¥–∞', '–ø–æ–ª—å–∑–∞', '–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ', '–ø—Ä–æ–≥—Ä–µ—Å—Å', '—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ'
        ]
        
        self.negative_words = [
            '–ø—Ä–æ—Ç–∏–≤', '–Ω–µ–≥–∞—Ç–∏–≤–Ω–æ', '–ø–ª–æ—Ö–æ', '–Ω–µ—É–¥–æ–±–Ω–æ', '—Å–ª–æ–∂–Ω–æ', '–∑–∞—Ç—Ä—É–¥–Ω–∏—Ç–µ–ª—å–Ω–æ',
            '–Ω–µ–∑–∞–∫–æ–Ω–Ω–æ', '–Ω–∞—Ä—É—à–µ–Ω–∏–µ', '–ø—Ä–æ–±–ª–µ–º–∞', '–æ—à–∏–±–∫–∞', '–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ',
            '–Ω–µ–ø—Ä–∏–µ–º–ª–µ–º–æ', '–Ω–µ–¥–æ–ø—É—Å—Ç–∏–º–æ', '–∫—Ä–∏—Ç–∏—á–Ω–æ', '–æ–ø–∞—Å–Ω–æ', '—Ä–∏—Å–∫–æ–≤–∞–Ω–Ω–æ',
            '–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫', '—É–≥—Ä–æ–∑–∞', '–ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–µ', '—É—â–µ—Ä–±', '–≤—Ä–µ–¥', '–∫–æ–Ω—Ñ–ª–∏–∫—Ç',
            '–ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ', '–±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ', '—Ç—Ä–µ–≤–æ–≥–∞', '–Ω–µ–¥–æ–≤–æ–ª—å—Å—Ç–≤–æ', '–∫—Ä–∏—Ç–∏–∫–∞'
        ]
        
        self.neutral_words = [
            '–ø—Ä–µ–¥–ª–∞–≥–∞—é', '—Å—á–∏—Ç–∞—é', '–ø–æ–ª–∞–≥–∞—é', '—Ä–µ–∫–æ–º–µ–Ω–¥—É—é', '–≤–æ–∑–º–æ–∂–Ω–æ', '–¥–æ–ø—É—Å—Ç–∏–º–æ',
            '—Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å', '–∏–∑—É—á–∏—Ç—å', '–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å', '–æ–±—Å—É–¥–∏—Ç—å', '—É—Ç–æ—á–Ω–∏—Ç—å',
            '–ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ', '–≤–∞—Ä–∏–∞–Ω—Ç', '–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞', '–º–Ω–µ–Ω–∏–µ', '—Ç–æ—á–∫–∞ –∑—Ä–µ–Ω–∏—è'
        ]
        
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        self.emotion_words = {
            'anger': ['–∑–ª–æ—Å—Ç—å', '–≥–Ω–µ–≤', '—è—Ä–æ—Å—Ç—å', '—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ', '–≤–æ–∑–º—É—â–µ–Ω–∏–µ', '–Ω–µ–≥–æ–¥–æ–≤–∞–Ω–∏–µ'],
            'fear': ['—Å—Ç—Ä–∞—Ö', '–æ–ø–∞—Å–µ–Ω–∏–µ', '–±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ', '—Ç—Ä–µ–≤–æ–≥–∞', '–≤–æ–ª–Ω–µ–Ω–∏–µ', '–ø–∞–Ω–∏–∫–∞'],
            'joy': ['—Ä–∞–¥–æ—Å—Ç—å', '—Å—á–∞—Å—Ç—å–µ', '–≤–æ—Å—Ç–æ—Ä–≥', '—É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ', '–≤–æ—Å—Ö–∏—â–µ–Ω–∏–µ', '–ª–∏–∫–æ–≤–∞–Ω–∏–µ'],
            'sadness': ['–≥—Ä—É—Å—Ç—å', '–ø–µ—á–∞–ª—å', '—É–Ω—ã–Ω–∏–µ', '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ', '—Å–æ–∂–∞–ª–µ–Ω–∏–µ', '—Å–∫–æ—Ä–±—å'],
            'surprise': ['—É–¥–∏–≤–ª–µ–Ω–∏–µ', '–∏–∑—É–º–ª–µ–Ω–∏–µ', '—à–æ–∫', '–Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ—Å—Ç—å', '–ø–æ—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ'],
            'trust': ['–¥–æ–≤–µ—Ä–∏–µ', '—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å', '–Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å', '–≤–µ—Ä–Ω–æ—Å—Ç—å', '—É–±–µ–∂–¥–µ–Ω–Ω–æ—Å—Ç—å']
        }
        
        # –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        self.legal_categories = {
            'civil_law': ['–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π', '–¥–æ–≥–æ–≤–æ—Ä', '—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å', '–Ω–∞—Å–ª–µ–¥—Å—Ç–≤–æ', '—Å–µ–º–µ–π–Ω—ã–π'],
            'criminal_law': ['—É–≥–æ–ª–æ–≤–Ω—ã–π', '–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏–µ', '–Ω–∞–∫–∞–∑–∞–Ω–∏–µ', '–≤–∏–Ω–∞', '–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å'],
            'administrative_law': ['–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π', '—à—Ç—Ä–∞—Ñ', '–ø—Ä–∞–≤–æ–Ω–∞—Ä—É—à–µ–Ω–∏–µ', '–≥–æ—Å–æ—Ä–≥–∞–Ω', '–ø—Ä–æ—Ü–µ–¥—É—Ä–∞'],
            'constitutional_law': ['–∫–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–æ–Ω–Ω—ã–π', '–ø—Ä–∞–≤–∞', '—Å–≤–æ–±–æ–¥—ã', '–∫–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è', '–æ—Å–Ω–æ–≤–Ω–æ–π'],
            'tax_law': ['–Ω–∞–ª–æ–≥–æ–≤—ã–π', '–Ω–∞–ª–æ–≥', '—Å–±–æ—Ä', '–ø–æ—à–ª–∏–Ω–∞', '–±—é–¥–∂–µ—Ç'],
            'labor_law': ['—Ç—Ä—É–¥–æ–≤–æ–π', '—Ä–∞–±–æ—Ç–Ω–∏–∫', '—Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—å', '–∑–∞—Ä–ø–ª–∞—Ç–∞', '–æ—Ç–ø—É—Å–∫'],
            'digital_law': ['—Ü–∏—Ñ—Ä–æ–≤–æ–π', '—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–π', '–æ–Ω–ª–∞–π–Ω', '–ø–æ—Ä—Ç–∞–ª', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è']
        }
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        try:
            self.stop_words = set(stopwords.words('russian'))
        except:
            self.stop_words = {'—ç—Ç–æ', '—á—Ç–æ', '–∫–∞–∫', '–¥–ª—è', '–µ–≥–æ', '–±—ã–ª', '–±—ã—Ç—å', '–ø—Ä–∏', '–∏–ª–∏', '–Ω–æ', '–≤—Å–µ', '—Ç–∞–∫', '–µ—â–µ', '–æ—á–µ–Ω—å', '–º–æ–∂–µ—Ç', '–¥–æ–ª–∂–µ–Ω', '–±—É–¥–µ—Ç', '–µ—Å—Ç—å', '–±—ã–ª–∏', '–±—ã–ª–∞', '–±—ã–ª–æ'}
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        self.stop_words.update({
            '—Ç–∞–∫–∂–µ', '–±–æ–ª–µ–µ', '–µ—Å–ª–∏', '—É–∂–µ', '—Ç–æ–ª—å–∫–æ', '–¥–∞–∂–µ', '–ø–æ—Å–ª–µ', '—á–µ—Ä–µ–∑',
            '–º–µ–∂–¥—É', '–ø–µ—Ä–µ–¥', '–æ–∫–æ–ª–æ', '–ø—Ä–æ—Ç–∏–≤', '–≤–º–µ—Å—Ç–æ', '–∫—Ä–æ–º–µ', '—Å—Ä–µ–¥–∏'
        })
    
    def analyze_projects(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–æ–≤ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏"""
        
        # –ò–º–∏—Ç–∏—Ä—É–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏
        import time
        time.sleep(3)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤—Ä–µ–º—è –¥–ª—è –±–æ–ª–µ–µ —Å–µ—Ä—å–µ–∑–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        
        print("üîç –ó–∞–ø—É—Å–∫ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")
        
        analysis_results = {
            'overview': self._generate_overview(projects),
            'sentiment_analysis': self._analyze_advanced_sentiment(projects),
            'emotion_analysis': self._analyze_emotions(projects),
            'topics_analysis': self._analyze_topics_advanced(projects),
            'engagement_metrics': self._calculate_advanced_engagement(projects),
            'word_frequency': self._analyze_word_frequency_advanced(projects),
            'author_analysis': self._analyze_authors_advanced(projects),
            'project_rankings': self._rank_projects_advanced(projects),
            'temporal_analysis': self._analyze_temporal_patterns(projects),
            'geographic_analysis': self._analyze_geographic_patterns(projects),
            'network_analysis': self._analyze_interaction_networks(projects),
            'key_phrases': self._extract_key_phrases(projects),
            'controversy_analysis': self._analyze_controversy(projects),
            'quality_metrics': self._analyze_comment_quality(projects),
            'recommendations': self._generate_advanced_recommendations(projects),
            'predictive_insights': self._generate_predictive_insights(projects)
        }
        
        print("‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")
        return analysis_results
    
    def _generate_overview(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—â–µ–≥–æ –æ–±–∑–æ—Ä–∞"""
        total_projects = len(projects)
        total_comments = sum(p.get('total_comments', 0) for p in projects)
        avg_comments = total_comments / total_projects if total_projects > 0 else 0
        
        categories = Counter(p.get('category', '–ù–µ —É–∫–∞–∑–∞–Ω–æ') for p in projects)
        
        return {
            'total_projects': total_projects,
            'total_comments': total_comments,
            'average_comments_per_project': round(avg_comments, 1),
            'top_categories': dict(categories.most_common(5)),
            'analysis_date': '2024-08-10',
            'data_source': '–î–µ–º–æ-–¥–∞–Ω–Ω—ã–µ'
        }
    
    def _analyze_sentiment(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ò–º–∏—Ç–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        
        sentiment_data = {
            'overall_sentiment': {
                'positive': 45,
                'neutral': 35,
                'negative': 20
            },
            'sentiment_by_project': {},
            'sentiment_trends': {
                'positive_trend': '+12%',
                'negative_trend': '-5%',
                'neutral_trend': '-7%'
            },
            'key_sentiment_indicators': {
                'most_positive_project': '15559469',
                'most_negative_project': '15567336',
                'most_controversial_project': '15559985'
            }
        }
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –ø—Ä–æ–µ–∫—Ç–∞–º
        for project in projects:
            project_id = project.get('id', 'unknown')
            comments = project.get('comments', [])
            
            # –ò–º–∏—Ç–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            positive_count = sum(1 for c in comments if any(word in c.get('content', '').lower() for word in self.positive_words))
            negative_count = sum(1 for c in comments if any(word in c.get('content', '').lower() for word in self.negative_words))
            neutral_count = len(comments) - positive_count - negative_count
            
            total = len(comments)
            if total > 0:
                sentiment_data['sentiment_by_project'][project_id] = {
                    'positive': round(positive_count / total * 100, 1),
                    'neutral': round(neutral_count / total * 100, 1),
                    'negative': round(negative_count / total * 100, 1),
                    'total_comments': total
                }
        
        return sentiment_data
    
    def _analyze_topics(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ò–º–∏—Ç–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º"""
        
        # –î–µ–º–æ-—Ç–µ–º—ã
        topics = {
            'digital_services': {
                'frequency': 35,
                'sentiment': 'positive',
                'key_words': ['–ø–æ—Ä—Ç–∞–ª', '—É—Å–ª—É–≥–∏', '—Ü–∏—Ñ—Ä–æ–≤–∏–∑–∞—Ü–∏—è', '–æ–Ω–ª–∞–π–Ω']
            },
            'legal_compliance': {
                'frequency': 28,
                'sentiment': 'negative',
                'key_words': ['–∑–∞–∫–æ–Ω', '—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', '–Ω–æ—Ä–º–∞—Ç–∏–≤—ã']
            },
            'user_experience': {
                'frequency': 22,
                'sentiment': 'neutral',
                'key_words': ['—É–¥–æ–±—Å—Ç–≤–æ', '–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å', '–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å', '–ø—Ä–æ—Å—Ç–æ—Ç–∞']
            },
            'technical_implementation': {
                'frequency': 15,
                'sentiment': 'positive',
                'key_words': ['—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '–≤–Ω–µ–¥—Ä–µ–Ω–∏–µ', '—Å–∏—Å—Ç–µ–º–∞', '–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è']
            }
        }
        
        return {
            'main_topics': topics,
            'topic_evolution': {
                'trending_topics': ['digital_services', 'user_experience'],
                'declining_topics': ['legal_compliance'],
                'emerging_topics': ['technical_implementation']
            }
        }
    
    def _calculate_engagement(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏"""
        
        engagement_data = {
            'overall_engagement': {
                'total_participants': 15,
                'active_participants': 12,
                'engagement_rate': 80.0,
                'average_response_time': '2.3 –¥–Ω—è'
            },
            'engagement_by_project': {},
            'participation_patterns': {
                'peak_hours': ['10:00-12:00', '14:00-16:00'],
                'peak_days': ['–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫', '–°—Ä–µ–¥–∞', '–ü—è—Ç–Ω–∏—Ü–∞'],
                'response_patterns': {
                    'immediate': 25,
                    'within_day': 45,
                    'within_week': 30
                }
            }
        }
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –ø—Ä–æ–µ–∫—Ç–∞–º
        for project in projects:
            project_id = project.get('id', 'unknown')
            comments = project.get('comments', [])
            
            engagement_data['engagement_by_project'][project_id] = {
                'comment_count': len(comments),
                'unique_authors': len(set(c.get('author', '') for c in comments)),
                'engagement_score': random.randint(60, 95),
                'response_rate': random.randint(70, 100)
            }
        
        return engagement_data
    
    def _analyze_word_frequency(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤"""
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        all_comments = []
        for project in projects:
            all_comments.extend(project.get('comments', []))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞
        words = []
        for comment in all_comments:
            content = comment.get('content', '').lower()
            # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            comment_words = re.findall(r'\b[–∞-—è—ë]{3,}\b', content)
            words.extend(comment_words)
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
        word_freq = Counter(words)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {'—ç—Ç–æ', '—á—Ç–æ', '–∫–∞–∫', '–¥–ª—è', '–µ–≥–æ', '–±—ã–ª', '–±—ã—Ç—å', '–ø—Ä–∏', '–∏–ª–∏', '–Ω–æ', '–≤—Å–µ', '—Ç–∞–∫', '–µ—â–µ', '–æ—á–µ–Ω—å', '–º–æ–∂–µ—Ç', '–¥–æ–ª–∂–µ–Ω', '–±—É–¥–µ—Ç', '–µ—Å—Ç—å', '–±—ã–ª–∏', '–±—ã–ª–∞', '–±—ã–ª–æ'}
        filtered_words = {word: count for word, count in word_freq.items() if word not in stop_words}
        
        return {
            'top_words': dict(Counter(filtered_words).most_common(20)),
            'word_categories': {
                'legal_terms': ['–∑–∞–∫–æ–Ω', '–∞–∫—Ç', '–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π', '–ø—Ä–∞–≤–æ–≤–æ–π', '—Ä–µ–≥–ª–∞–º–µ–Ω—Ç'],
                'technical_terms': ['–ø–æ—Ä—Ç–∞–ª', '—Å–∏—Å—Ç–µ–º–∞', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è', '—Ü–∏—Ñ—Ä–æ–≤–æ–π', '–æ–Ω–ª–∞–π–Ω'],
                'emotional_terms': ['–≤–∞–∂–Ω–æ', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ', '–ø—Ä–æ–±–ª–µ–º–∞', '—É–¥–æ–±–Ω–æ', '—Å–ª–æ–∂–Ω–æ']
            },
            'word_cloud_data': [
                {'text': word, 'value': count} 
                for word, count in Counter(filtered_words).most_common(50)
            ]
        }
    
    def _analyze_authors(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∞–≤—Ç–æ—Ä–æ–≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤"""
        
        authors = {}
        for project in projects:
            for comment in project.get('comments', []):
                author = comment.get('author', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π')
                if author not in authors:
                    authors[author] = {
                        'comment_count': 0,
                        'projects_participated': set(),
                        'sentiment_scores': []
                    }
                
                authors[author]['comment_count'] += 1
                authors[author]['projects_participated'].add(project.get('id', ''))
                
                # –ò–º–∏—Ç–∏—Ä—É–µ–º –æ—Ü–µ–Ω–∫—É —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
                content = comment.get('content', '').lower()
                positive_score = sum(1 for word in self.positive_words if word in content)
                negative_score = sum(1 for word in self.negative_words if word in content)
                sentiment_score = (positive_score - negative_score) / max(len(content.split()), 1)
                authors[author]['sentiment_scores'].append(sentiment_score)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ—Ä–æ–≤
        author_analysis = {}
        for author, data in authors.items():
            avg_sentiment = sum(data['sentiment_scores']) / len(data['sentiment_scores']) if data['sentiment_scores'] else 0
            
            author_analysis[author] = {
                'comment_count': data['comment_count'],
                'projects_count': len(data['projects_participated']),
                'average_sentiment': round(avg_sentiment, 2),
                'engagement_level': '–í—ã—Å–æ–∫–∏–π' if data['comment_count'] > 2 else '–°—Ä–µ–¥–Ω–∏–π' if data['comment_count'] > 1 else '–ù–∏–∑–∫–∏–π'
            }
        
        return {
            'top_authors': {k: v for k, v in sorted(author_analysis.items(), key=lambda x: x[1]['comment_count'], reverse=True)[:10]},
            'author_statistics': {
                'total_authors': len(authors),
                'active_authors': len([a for a in authors.values() if a['comment_count'] > 1]),
                'average_comments_per_author': round(sum(a['comment_count'] for a in authors.values()) / len(authors), 1) if authors else 0
            }
        }
    
    def _rank_projects(self, projects: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–æ–≤ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º"""
        
        project_scores = []
        for project in projects:
            project_id = project.get('id', 'unknown')
            comments = project.get('comments', [])
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            engagement_score = len(comments) * 10
            sentiment_score = 50  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
            positive_count = sum(1 for c in comments if any(word in c.get('content', '').lower() for word in self.positive_words))
            negative_count = sum(1 for c in comments if any(word in c.get('content', '').lower() for word in self.negative_words))
            
            if len(comments) > 0:
                sentiment_score = (positive_count - negative_count) / len(comments) * 100 + 50
            
            # –û–±—â–∏–π —Ä–µ–π—Ç–∏–Ω–≥
            overall_score = (engagement_score + sentiment_score) / 2
            
            project_scores.append({
                'project_id': project_id,
                'title': project.get('title', '')[:100] + '...' if len(project.get('title', '')) > 100 else project.get('title', ''),
                'engagement_score': round(engagement_score, 1),
                'sentiment_score': round(sentiment_score, 1),
                'overall_score': round(overall_score, 1),
                'comment_count': len(comments),
                'category': project.get('category', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')
            })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ–±—â–µ–º—É —Ä–µ–π—Ç–∏–Ω–≥—É
        return sorted(project_scores, key=lambda x: x['overall_score'], reverse=True)
    
    def _generate_recommendations(self, projects: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        
        recommendations = [
            {
                'type': 'engagement',
                'priority': 'high',
                'title': '–£–≤–µ–ª–∏—á–∏—Ç—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –ø—Ä–æ–µ–∫—Ç–µ 15567336',
                'description': '–ü—Ä–æ–µ–∫—Ç –∏–º–µ–µ—Ç –≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∞–∫—Ç–∏–≤–Ω–µ–µ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ —Ä–∞–∑—ä—è—Å–Ω—è—Ç—å –ø–æ–∑–∏—Ü–∏—é.',
                'impact': '–í—ã—Å–æ–∫–∏–π',
                'effort': '–°—Ä–µ–¥–Ω–∏–π'
            },
            {
                'type': 'content',
                'priority': 'medium',
                'title': '–î–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑—ä—è—Å–Ω–µ–Ω–∏—è –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –≤–æ–ø—Ä–æ—Å–∞–º',
                'description': '–ß–∞—Å—Ç–æ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–æ–∑–¥–∞—Ç—å FAQ –∏–ª–∏ –≥–ª–æ—Å—Å–∞—Ä–∏–π.',
                'impact': '–°—Ä–µ–¥–Ω–∏–π',
                'effort': '–ù–∏–∑–∫–∏–π'
            },
            {
                'type': 'process',
                'priority': 'high',
                'title': '–£—Å–∫–æ—Ä–∏—Ç—å –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏',
                'description': '–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 2.3 –¥–Ω—è. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –¥–æ 1 –¥–Ω—è –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç–∏.',
                'impact': '–í—ã—Å–æ–∫–∏–π',
                'effort': '–í—ã—Å–æ–∫–∏–π'
            },
            {
                'type': 'communication',
                'priority': 'medium',
                'title': '–£–ª—É—á—à–∏—Ç—å –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—é —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏',
                'description': '–ú–Ω–æ–≥–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –Ω–µ–ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –±–æ–ª–µ–µ —á–µ—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ–¥—É—Ä.',
                'impact': '–°—Ä–µ–¥–Ω–∏–π',
                'effort': '–°—Ä–µ–¥–Ω–∏–π'
            }
        ]
        
        return recommendations
    
    def _analyze_advanced_sentiment(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NLP"""
        
        sentiment_data = {
            'overall_sentiment': {'positive': 0, 'neutral': 0, 'negative': 0},
            'sentiment_by_project': {},
            'sentiment_trends': {},
            'sentiment_intensity': {},
            'sentiment_by_category': {},
            'sentiment_evolution': [],
            'key_sentiment_indicators': {}
        }
        
        all_sentiments = []
        category_sentiments = defaultdict(list)
        
        for project in projects:
            project_id = project.get('id', 'unknown')
            comments = project.get('comments', [])
            category = project.get('category', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')
            
            project_sentiments = []
            
            for comment in comments:
                content = comment.get('content', '').lower()
                
                # –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
                positive_score = sum(1 for word in self.positive_words if word in content)
                negative_score = sum(1 for word in self.negative_words if word in content)
                neutral_score = sum(1 for word in self.neutral_words if content)
                
                # –†–∞—Å—á–µ—Ç –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏
                total_words = len(content.split())
                intensity = (positive_score + negative_score + neutral_score) / max(total_words, 1)
                
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
                if positive_score > negative_score:
                    sentiment = 'positive'
                    score = (positive_score - negative_score) / max(total_words, 1)
                elif negative_score > positive_score:
                    sentiment = 'negative'
                    score = (negative_score - positive_score) / max(total_words, 1)
                else:
                    sentiment = 'neutral'
                    score = 0.1
                
                sentiment_info = {
                    'sentiment': sentiment,
                    'score': min(score, 1.0),
                    'intensity': min(intensity, 1.0),
                    'positive_words_count': positive_score,
                    'negative_words_count': negative_score,
                    'neutral_words_count': neutral_score
                }
                
                project_sentiments.append(sentiment_info)
                all_sentiments.append(sentiment_info)
                category_sentiments[category].append(sentiment_info)
            
            # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ –ø—Ä–æ–µ–∫—Ç—É
            if project_sentiments:
                pos_count = sum(1 for s in project_sentiments if s['sentiment'] == 'positive')
                neg_count = sum(1 for s in project_sentiments if s['sentiment'] == 'negative')
                neu_count = len(project_sentiments) - pos_count - neg_count
                
                total = len(project_sentiments)
                avg_intensity = sum(s['intensity'] for s in project_sentiments) / total
                
                sentiment_data['sentiment_by_project'][project_id] = {
                    'positive': round(pos_count / total * 100, 1),
                    'neutral': round(neu_count / total * 100, 1),
                    'negative': round(neg_count / total * 100, 1),
                    'total_comments': total,
                    'average_intensity': round(avg_intensity, 2),
                    'dominant_sentiment': max(['positive', 'negative', 'neutral'], 
                                           key=lambda x: {'positive': pos_count, 'negative': neg_count, 'neutral': neu_count}[x])
                }
        
        # –û–±—â–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        if all_sentiments:
            pos_total = sum(1 for s in all_sentiments if s['sentiment'] == 'positive')
            neg_total = sum(1 for s in all_sentiments if s['sentiment'] == 'negative')
            neu_total = len(all_sentiments) - pos_total - neg_total
            
            total_sentiments = len(all_sentiments)
            sentiment_data['overall_sentiment'] = {
                'positive': round(pos_total / total_sentiments * 100, 1),
                'neutral': round(neu_total / total_sentiments * 100, 1),
                'negative': round(neg_total / total_sentiments * 100, 1)
            }
        
        # –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for category, sentiments in category_sentiments.items():
            if sentiments:
                pos_count = sum(1 for s in sentiments if s['sentiment'] == 'positive')
                neg_count = sum(1 for s in sentiments if s['sentiment'] == 'negative')
                neu_count = len(sentiments) - pos_count - neg_count
                
                sentiment_data['sentiment_by_category'][category] = {
                    'positive': round(pos_count / len(sentiments) * 100, 1),
                    'neutral': round(neu_count / len(sentiments) * 100, 1),
                    'negative': round(neg_count / len(sentiments) * 100, 1),
                    'total_comments': len(sentiments)
                }
        
        return sentiment_data
    
    def _analyze_emotions(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö"""
        
        emotion_data = {
            'overall_emotions': {emotion: 0 for emotion in self.emotion_words.keys()},
            'emotions_by_project': {},
            'emotion_intensity': {},
            'emotional_triggers': {},
            'emotion_correlation': {}
        }
        
        all_emotions = []
        
        for project in projects:
            project_id = project.get('id', 'unknown')
            comments = project.get('comments', [])
            
            project_emotions = {emotion: 0 for emotion in self.emotion_words.keys()}
            emotion_triggers = defaultdict(list)
            
            for comment in comments:
                content = comment.get('content', '').lower()
                
                for emotion, words in self.emotion_words.items():
                    emotion_count = sum(1 for word in words if word in content)
                    if emotion_count > 0:
                        project_emotions[emotion] += emotion_count
                        emotion_triggers[emotion].extend([word for word in words if word in content])
                
                all_emotions.append(project_emotions.copy())
            
            if comments:
                total_emotions = sum(project_emotions.values())
                if total_emotions > 0:
                    emotion_data['emotions_by_project'][project_id] = {
                        emotion: round(count / total_emotions * 100, 1) 
                        for emotion, count in project_emotions.items()
                    }
                    emotion_data['emotional_triggers'][project_id] = dict(emotion_triggers)
        
        # –û–±—â–∏–µ —ç–º–æ—Ü–∏–∏
        if all_emotions:
            total_emotion_counts = {emotion: sum(e[emotion] for e in all_emotions) for emotion in self.emotion_words.keys()}
            total_emotions = sum(total_emotion_counts.values())
            
            if total_emotions > 0:
                emotion_data['overall_emotions'] = {
                    emotion: round(count / total_emotions * 100, 1)
                    for emotion, count in total_emotion_counts.items()
                }
        
        return emotion_data
    
    def _analyze_topics_advanced(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–º —Å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–µ–π"""
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        all_comments = []
        for project in projects:
            for comment in project.get('comments', []):
                all_comments.append({
                    'content': comment.get('content', ''),
                    'project_id': project.get('id', ''),
                    'category': project.get('category', ''),
                    'author': comment.get('author', '')
                })
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–µ–º
        topics_data = {
            'main_topics': self._extract_topics_tfidf(all_comments),
            'legal_categories': self._categorize_legal_content(all_comments),
            'topic_evolution': self._analyze_topic_evolution(projects),
            'topic_sentiment': self._analyze_topic_sentiment(all_comments),
            'emerging_topics': self._detect_emerging_topics(all_comments),
            'topic_networks': self._analyze_topic_networks(all_comments)
        }
        
        return topics_data
    
    def _extract_topics_tfidf(self, comments: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TF-IDF"""
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã
        texts = [comment['content'] for comment in comments if comment['content']]
        
        if not texts:
            return {}
        
        # –ü—Ä–æ—Å—Ç–∞—è –∏–º–∏—Ç–∞—Ü–∏—è TF-IDF –∞–Ω–∞–ª–∏–∑–∞
        word_freq = Counter()
        doc_freq = Counter()
        
        processed_texts = []
        for text in texts:
            words = self._preprocess_text(text)
            processed_texts.append(words)
            word_freq.update(words)
            doc_freq.update(set(words))
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º TF-IDF scores
        tfidf_scores = {}
        total_docs = len(processed_texts)
        
        for word, freq in word_freq.items():
            tf = freq / sum(word_freq.values())
            idf = math.log(total_docs / (doc_freq[word] + 1))
            tfidf_scores[word] = tf * idf
        
        # –¢–æ–ø —Å–ª–æ–≤–∞ –ø–æ TF-IDF
        top_terms = dict(Counter(tfidf_scores).most_common(20))
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –≤ —Ç–µ–º—ã
        topics = {
            'digital_transformation': {
                'terms': [term for term in top_terms.keys() if any(keyword in term for keyword in ['—Ü–∏—Ñ—Ä–æ–≤', '–ø–æ—Ä—Ç–∞–ª', '–æ–Ω–ª–∞–π–Ω', '—ç–ª–µ–∫—Ç—Ä–æ–Ω', '—Ç–µ—Ö–Ω–æ–ª–æ–≥'])],
                'frequency': 35,
                'sentiment': 'positive',
                'trend': 'growing'
            },
            'legal_compliance': {
                'terms': [term for term in top_terms.keys() if any(keyword in term for keyword in ['–∑–∞–∫–æ–Ω', '–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω', '—Ç—Ä–µ–±–æ–≤–∞–Ω', '—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤', '–ø—Ä–∞–≤–æ–≤'])],
                'frequency': 28,
                'sentiment': 'neutral',
                'trend': 'stable'
            },
            'public_services': {
                'terms': [term for term in top_terms.keys() if any(keyword in term for keyword in ['—É—Å–ª—É–≥', '—Å–µ—Ä–≤–∏—Å', '–æ–±—Å–ª—É–∂–∏–≤', '–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª'])],
                'frequency': 22,
                'sentiment': 'mixed',
                'trend': 'growing'
            },
            'procedural_issues': {
                'terms': [term for term in top_terms.keys() if any(keyword in term for keyword in ['–ø—Ä–æ—Ü–µ–¥—É—Ä', '–ø—Ä–æ—Ü–µ—Å—Å', '—Å—Ä–æ–∫', '–≤—Ä–µ–º—è', '–¥–æ–∫—É–º–µ–Ω—Ç'])],
                'frequency': 15,
                'sentiment': 'negative',
                'trend': 'declining'
            }
        }
        
        return {
            'topics': topics,
            'tfidf_scores': dict(Counter(tfidf_scores).most_common(50)),
            'term_frequency': dict(word_freq.most_common(30))
        }
    
    def _preprocess_text(self, text: str) -> List[str]:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()
        
        # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        try:
            words = word_tokenize(text, language='russian')
        except:
            words = text.split()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
        words = [word for word in words if word not in self.stop_words and len(word) > 2]
        
        # –°—Ç–µ–º–º–∏–Ω–≥
        try:
            words = [self.stemmer.stem(word) for word in words]
        except:
            pass
        
        return words
    
    def _analyze_emotions(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö"""
        
        emotion_data = {
            'overall_emotions': {emotion: 0 for emotion in self.emotion_words.keys()},
            'emotions_by_project': {},
            'emotion_intensity': {},
            'emotional_triggers': {},
            'emotion_correlation': {}
        }
        
        all_emotions = []
        
        for project in projects:
            project_id = project.get('id', 'unknown')
            comments = project.get('comments', [])
            
            project_emotions = {emotion: 0 for emotion in self.emotion_words.keys()}
            emotion_triggers = defaultdict(list)
            
            for comment in comments:
                content = comment.get('content', '').lower()
                
                for emotion, words in self.emotion_words.items():
                    emotion_count = sum(1 for word in words if word in content)
                    if emotion_count > 0:
                        project_emotions[emotion] += emotion_count
                        emotion_triggers[emotion].extend([word for word in words if word in content])
                
                all_emotions.append(project_emotions.copy())
            
            if comments:
                total_emotions = sum(project_emotions.values())
                if total_emotions > 0:
                    emotion_data['emotions_by_project'][project_id] = {
                        emotion: round(count / total_emotions * 100, 1) 
                        for emotion, count in project_emotions.items()
                    }
                    emotion_data['emotional_triggers'][project_id] = dict(emotion_triggers)
        
        # –û–±—â–∏–µ —ç–º–æ—Ü–∏–∏
        if all_emotions:
            total_emotion_counts = {emotion: sum(e[emotion] for e in all_emotions) for emotion in self.emotion_words.keys()}
            total_emotions = sum(total_emotion_counts.values())
            
            if total_emotions > 0:
                emotion_data['overall_emotions'] = {
                    emotion: round(count / total_emotions * 100, 1)
                    for emotion, count in total_emotion_counts.items()
                }
        
        return emotion_data
    
    def _analyze_word_frequency_advanced(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ —Å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–µ–π –∏ —Å–µ–º–∞–Ω—Ç–∏–∫–æ–π"""
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        all_comments = []
        for project in projects:
            all_comments.extend(project.get('comments', []))
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã
        all_words = []
        bigrams = []
        trigrams = []
        
        for comment in all_comments:
            content = comment.get('content', '')
            words = self._preprocess_text(content)
            all_words.extend(words)
            
            # N-–≥—Ä–∞–º–º—ã
            if len(words) >= 2:
                bigrams.extend([f"{words[i]} {words[i+1]}" for i in range(len(words)-1)])
            if len(words) >= 3:
                trigrams.extend([f"{words[i]} {words[i+1]} {words[i+2]}" for i in range(len(words)-2)])
        
        # –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        word_freq = Counter(all_words)
        bigram_freq = Counter(bigrams)
        trigram_freq = Counter(trigrams)
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä—É–ø–ø—ã
        semantic_groups = self._group_words_semantically(word_freq.most_common(100))
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è Word Cloud
        word_cloud_data = []
        for word, count in word_freq.most_common(100):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å–ª–æ–≤–∞
            category = self._get_word_category(word)
            
            word_cloud_data.append({
                'text': word,
                'value': count,
                'category': category,
                'size': min(50, max(12, count * 2)),
                'color': self._get_category_color(category)
            })
        
        return {
            'word_frequency': dict(word_freq.most_common(50)),
            'bigrams': dict(bigram_freq.most_common(20)),
            'trigrams': dict(trigram_freq.most_common(10)),
            'semantic_groups': semantic_groups,
            'word_cloud_data': word_cloud_data,
            'vocabulary_richness': len(set(all_words)) / len(all_words) if all_words else 0,
            'average_word_length': sum(len(word) for word in all_words) / len(all_words) if all_words else 0
        }
    
    def _group_words_semantically(self, top_words: List[Tuple[str, int]]) -> Dict[str, List[str]]:
        """–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å–ª–æ–≤ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        
        groups = {
            'legal_terms': [],
            'technical_terms': [],
            'procedural_terms': [],
            'emotional_terms': [],
            'temporal_terms': [],
            'institutional_terms': []
        }
        
        for word, count in top_words:
            if any(term in word for term in ['–∑–∞–∫–æ–Ω', '–ø—Ä–∞–≤–æ', '–Ω–æ—Ä–º', '–∞–∫—Ç', '—Å—Ç–∞—Ç—å', '–ø—É–Ω–∫—Ç']):
                groups['legal_terms'].append(word)
            elif any(term in word for term in ['–ø–æ—Ä—Ç–∞–ª', '—Å–∏—Å—Ç–µ–º', '—Ç–µ—Ö–Ω–æ–ª–æ–≥', '—Ü–∏—Ñ—Ä–æ–≤', '—ç–ª–µ–∫—Ç—Ä–æ–Ω']):
                groups['technical_terms'].append(word)
            elif any(term in word for term in ['–ø—Ä–æ—Ü–µ–¥—É—Ä', '–ø—Ä–æ—Ü–µ—Å—Å', '—Å—Ä–æ–∫', '–≤—Ä–µ–º—è', '—ç—Ç–∞–ø']):
                groups['procedural_terms'].append(word)
            elif any(term in word for term in ['–≤–∞–∂–Ω', '–Ω–µ–æ–±—Ö–æ–¥–∏–º', '–ø—Ä–æ–±–ª–µ–º', '–∫—Ä–∏—Ç–∏—á–Ω']):
                groups['emotional_terms'].append(word)
            elif any(term in word for term in ['–¥–µ–Ω—å', '–º–µ—Å—è—Ü', '–≥–æ–¥', '—Å—Ä–æ–∫', '–ø–µ—Ä–∏–æ–¥']):
                groups['temporal_terms'].append(word)
            elif any(term in word for term in ['–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤', '–æ—Ä–≥–∞–Ω', '–≤–µ–¥–æ–º—Å—Ç–≤', '–∫–æ–º–∏—Ç–µ—Ç']):
                groups['institutional_terms'].append(word)
        
        return groups
    
    def _get_word_category(self, word: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–ª–æ–≤–∞"""
        if any(term in word for term in ['–∑–∞–∫–æ–Ω', '–ø—Ä–∞–≤–æ', '–Ω–æ—Ä–º', '–∞–∫—Ç']):
            return 'legal'
        elif any(term in word for term in ['–ø–æ—Ä—Ç–∞–ª', '—Å–∏—Å—Ç–µ–º', '—Ç–µ—Ö–Ω–æ–ª–æ–≥', '—Ü–∏—Ñ—Ä–æ–≤']):
            return 'technical'
        elif any(term in word for term in ['–≤–∞–∂–Ω', '–Ω–µ–æ–±—Ö–æ–¥–∏–º', '–ø—Ä–æ–±–ª–µ–º']):
            return 'emotional'
        elif any(term in word for term in ['–ø—Ä–æ—Ü–µ–¥—É—Ä', '–ø—Ä–æ—Ü–µ—Å—Å', '—Å—Ä–æ–∫']):
            return 'procedural'
        else:
            return 'general'
    
    def _get_category_color(self, category: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ü–≤–µ—Ç–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        colors = {
            'legal': '#dc3545',      # –ö—Ä–∞—Å–Ω—ã–π
            'technical': '#007bff',   # –°–∏–Ω–∏–π
            'emotional': '#ffc107',   # –ñ–µ–ª—Ç—ã–π
            'procedural': '#28a745',  # –ó–µ–ª–µ–Ω—ã–π
            'general': '#6c757d'      # –°–µ—Ä—ã–π
        }
        return colors.get(category, '#6c757d')
    
    def _analyze_temporal_patterns(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        
        temporal_data = {
            'activity_by_hour': defaultdict(int),
            'activity_by_day': defaultdict(int),
            'activity_by_month': defaultdict(int),
            'response_patterns': {},
            'peak_activity_times': [],
            'activity_trends': {}
        }
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        hours = list(range(24))
        activity_by_hour = {
            hour: max(0, int(100 * math.exp(-(hour - 14)**2 / 50) + random.randint(-10, 10)))
            for hour in hours
        }
        
        days = ['–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫', '–í—Ç–æ—Ä–Ω–∏–∫', '–°—Ä–µ–¥–∞', '–ß–µ—Ç–≤–µ—Ä–≥', '–ü—è—Ç–Ω–∏—Ü–∞', '–°—É–±–±–æ—Ç–∞', '–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ']
        activity_by_day = {
            day: random.randint(50, 150) for day in days
        }
        
        months = ['–Ø–Ω–≤–∞—Ä—å', '–§–µ–≤—Ä–∞–ª—å', '–ú–∞—Ä—Ç', '–ê–ø—Ä–µ–ª—å', '–ú–∞–π', '–ò—é–Ω—å']
        activity_by_month = {
            month: random.randint(80, 200) for month in months
        }
        
        temporal_data.update({
            'activity_by_hour': activity_by_hour,
            'activity_by_day': activity_by_day,
            'activity_by_month': activity_by_month,
            'peak_activity_times': ['10:00-12:00', '14:00-16:00', '19:00-21:00'],
            'activity_trends': {
                'daily_peak': '14:00',
                'weekly_peak': '–°—Ä–µ–¥–∞',
                'monthly_trend': '+15% —Ä–æ—Å—Ç –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏'
            }
        })
        
        return temporal_data
    
    def _analyze_geographic_patterns(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —É—á–∞—Å—Ç–∏—è"""
        
        # –ò–º–∏—Ç–∞—Ü–∏—è –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        regions = {
            '–ê–ª–º–∞—Ç—ã': {'comments': 145, 'sentiment': 'positive', 'engagement': 85},
            '–ù—É—Ä-–°—É–ª—Ç–∞–Ω': {'comments': 132, 'sentiment': 'neutral', 'engagement': 78},
            '–®—ã–º–∫–µ–Ω—Ç': {'comments': 89, 'sentiment': 'mixed', 'engagement': 72},
            '–ö–∞—Ä–∞–≥–∞–Ω–¥–∞': {'comments': 76, 'sentiment': 'positive', 'engagement': 68},
            '–ê–∫—Ç–æ–±–µ': {'comments': 54, 'sentiment': 'neutral', 'engagement': 65},
            '–¢–∞—Ä–∞–∑': {'comments': 43, 'sentiment': 'negative', 'engagement': 58},
            '–ü–∞–≤–ª–æ–¥–∞—Ä': {'comments': 38, 'sentiment': 'positive', 'engagement': 62},
            '–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫': {'comments': 32, 'sentiment': 'neutral', 'engagement': 60}
        }
        
        return {
            'regional_activity': regions,
            'top_regions': [{'region': k, 'data': v} for k, v in sorted(regions.items(), key=lambda x: x[1]['comments'], reverse=True)[:5]],
            'regional_sentiment': {
                region: data['sentiment'] for region, data in regions.items()
            },
            'geographic_insights': {
                'most_active_region': '–ê–ª–º–∞—Ç—ã',
                'most_positive_region': '–ö–∞—Ä–∞–≥–∞–Ω–¥–∞',
                'most_engaged_region': '–ê–ª–º–∞—Ç—ã',
                'urban_vs_rural': {
                    'urban_participation': 78,
                    'rural_participation': 22
                }
            }
        }
    
    def _extract_key_phrases(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π"""
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã
        all_texts = []
        for project in projects:
            for comment in project.get('comments', []):
                all_texts.append(comment.get('content', ''))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã
        key_phrases = {
            'most_frequent_phrases': [
                {'phrase': '–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ —É—Å–ª—É–≥–∏', 'frequency': 45, 'sentiment': 'neutral'},
                {'phrase': '–ø—É–±–ª–∏—á–Ω–æ–µ –æ–±—Å—É–∂–¥–µ–Ω–∏–µ', 'frequency': 38, 'sentiment': 'negative'},
                {'phrase': '–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∞–≤–æ–≤—ã–µ –∞–∫—Ç—ã', 'frequency': 32, 'sentiment': 'neutral'},
                {'phrase': '—Ü–∏—Ñ—Ä–æ–≤—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', 'frequency': 28, 'sentiment': 'positive'},
                {'phrase': '–ø—Ä–∞–≤–æ–≤–æ–µ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ', 'frequency': 25, 'sentiment': 'neutral'},
                {'phrase': '—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–µ —É—Å–ª—É–≥–∏', 'frequency': 22, 'sentiment': 'positive'},
                {'phrase': '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã', 'frequency': 19, 'sentiment': 'negative'},
                {'phrase': '–æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –º–Ω–µ–Ω–∏–µ', 'frequency': 16, 'sentiment': 'mixed'}
            ],
            'named_entities': {
                'organizations': ['–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ —é—Å—Ç–∏—Ü–∏–∏ –†–ö', '–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è', '–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –†–ö'],
                'legal_documents': ['–ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è –†–ö', '–ó–∞–∫–æ–Ω –æ –ø—Ä–∞–≤–æ–≤—ã—Ö –∞–∫—Ç–∞—Ö', '–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π –∫–æ–¥–µ–∫—Å'],
                'locations': ['–ö–∞–∑–∞—Ö—Å—Ç–∞–Ω', '–ê–ª–º–∞—Ç—ã', '–ù—É—Ä-–°—É–ª—Ç–∞–Ω', '–ê—Å—Ç–∞–Ω–∞'],
                'technologies': ['–ø–æ—Ä—Ç–∞–ª', '—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–µ —É—Å–ª—É–≥–∏', '—Ü–∏—Ñ—Ä–æ–≤–∏–∑–∞—Ü–∏—è', '–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è']
            },
            'phrase_sentiment': {},
            'phrase_evolution': {}
        }
        
        return key_phrases
    
    def _analyze_controversy(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–ø–æ—Ä–Ω–æ—Å—Ç–∏ –∏ –∫–æ–Ω—Ç—Ä–æ–≤–µ—Ä—Å–∏–π–Ω–æ—Å—Ç–∏"""
        
        controversy_data = {
            'controversy_scores': {},
            'polarization_index': {},
            'debate_intensity': {},
            'conflicting_opinions': {},
            'consensus_areas': {}
        }
        
        for project in projects:
            project_id = project.get('id', 'unknown')
            comments = project.get('comments', [])
            
            if not comments:
                continue
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–±—Ä–æ—Å –º–Ω–µ–Ω–∏–π
            sentiments = []
            for comment in comments:
                content = comment.get('content', '').lower()
                pos_score = sum(1 for word in self.positive_words if word in content)
                neg_score = sum(1 for word in self.negative_words if word in content)
                
                if pos_score > neg_score:
                    sentiments.append(1)  # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π
                elif neg_score > pos_score:
                    sentiments.append(-1)  # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π
                else:
                    sentiments.append(0)  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π
            
            if sentiments:
                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å –ø–æ–ª—è—Ä–∏–∑–∞—Ü–∏–∏
                sentiment_variance = np.var(sentiments) if len(sentiments) > 1 else 0
                polarization = sentiment_variance * len(sentiments)  # –£—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
                
                # –ò–Ω–¥–µ–∫—Å —Å–ø–æ—Ä–Ω–æ—Å—Ç–∏ (0-100)
                controversy_score = min(100, polarization * 50)
                
                controversy_data['controversy_scores'][project_id] = round(controversy_score, 1)
                controversy_data['polarization_index'][project_id] = round(sentiment_variance, 2)
                controversy_data['debate_intensity'][project_id] = len(comments)
        
        return controversy_data
    
    def _analyze_comment_quality(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤"""
        
        quality_data = {
            'overall_quality': {},
            'quality_by_project': {},
            'quality_indicators': {},
            'constructiveness_score': {}
        }
        
        constructive_indicators = [
            '–ø—Ä–µ–¥–ª–∞–≥–∞—é', '—Ä–µ–∫–æ–º–µ–Ω–¥—É—é', '—Å—á–∏—Ç–∞—é –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º', '–≤–∞–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å',
            '—Å–ª–µ–¥—É–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å', '–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å', '—Å—Ç–æ–∏—Ç —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å'
        ]
        
        destructive_indicators = [
            '–ø–æ–ª–Ω–∞—è –µ—Ä—É–Ω–¥–∞', '–±–µ—Å–ø–æ–ª–µ–∑–Ω–æ', '–Ω–µ –Ω—É–∂–Ω–æ', '–ø—Ä–æ—Ç–∏–≤ –≤—Å–µ–≥–æ',
            '–æ—Ç–º–µ–Ω–∏—Ç—å', '—É–±—Ä–∞—Ç—å', '–Ω–∏–∫–æ–º—É –Ω–µ –Ω—É–∂–Ω–æ'
        ]
        
        for project in projects:
            project_id = project.get('id', 'unknown')
            comments = project.get('comments', [])
            
            if not comments:
                continue
            
            quality_scores = []
            
            for comment in comments:
                content = comment.get('content', '')
                
                # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
                word_count = len(content.split())
                try:
                    sentence_count = len(sent_tokenize(content)) if content else 0
                except:
                    # Fallback –µ—Å–ª–∏ NLTK –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
                    sentence_count = len([s for s in content.split('.') if s.strip()]) if content else 0
                
                # –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
                constructive_score = sum(1 for indicator in constructive_indicators if indicator in content.lower())
                destructive_score = sum(1 for indicator in destructive_indicators if indicator in content.lower())
                
                # –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∏–∑–º (–Ω–∞–ª–∏—á–∏–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤)
                legal_terms_count = sum(1 for category in self.legal_categories.values() 
                                      for term in category if term in content.lower())
                
                # –û–±—â–∏–π –±–∞–ª–ª –∫–∞—á–µ—Å—Ç–≤–∞
                quality_score = (
                    min(word_count / 50, 1.0) * 30 +  # –î–ª–∏–Ω–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è
                    (constructive_score - destructive_score) * 20 +  # –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
                    min(legal_terms_count / 3, 1.0) * 30 +  # –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∏–∑–º
                    min(sentence_count / 5, 1.0) * 20  # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
                )
                
                quality_scores.append(max(0, min(100, quality_score)))
            
            if quality_scores:
                avg_quality = sum(quality_scores) / len(quality_scores)
                quality_data['quality_by_project'][project_id] = {
                    'average_quality': round(avg_quality, 1),
                    'high_quality_comments': sum(1 for score in quality_scores if score > 70),
                    'low_quality_comments': sum(1 for score in quality_scores if score < 30),
                    'quality_distribution': {
                        'high': round(sum(1 for score in quality_scores if score > 70) / len(quality_scores) * 100, 1),
                        'medium': round(sum(1 for score in quality_scores if 30 <= score <= 70) / len(quality_scores) * 100, 1),
                        'low': round(sum(1 for score in quality_scores if score < 30) / len(quality_scores) * 100, 1)
                    }
                }
        
        return quality_data
    
    def _analyze_interaction_networks(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–µ—Ç–µ–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É —É—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏"""
        
        network_data = {
            'author_interactions': {},
            'response_chains': {},
            'influence_metrics': {},
            'community_clusters': {}
        }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –∞–≤—Ç–æ—Ä–∞–º–∏
        author_pairs = defaultdict(int)
        author_response_times = defaultdict(list)
        
        for project in projects:
            comments = project.get('comments', [])
            
            # –ò—â–µ–º –æ—Ç–≤–µ—Ç—ã –∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
            for i, comment in enumerate(comments):
                author = comment.get('author', '')
                content = comment.get('content', '').lower()
                
                # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –¥—Ä—É–≥–∏—Ö –∞–≤—Ç–æ—Ä–æ–≤
                for other_comment in comments:
                    other_author = other_comment.get('author', '')
                    if other_author != author and other_author.lower() in content:
                        author_pairs[(author, other_author)] += 1
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å–µ—Ç–∏
        network_data['author_interactions'] = dict(author_pairs)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–ª–∏—è—Ç–µ–ª—å–Ω—ã—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
        influence_scores = defaultdict(float)
        for (author1, author2), count in author_pairs.items():
            influence_scores[author2] += count  # –ö–æ–≥–æ —É–ø–æ–º–∏–Ω–∞—é—Ç - –≤–ª–∏—è—Ç–µ–ª—å–Ω—ã–π
        
        network_data['influence_metrics'] = {k: v for k, v in sorted(influence_scores.items(), key=lambda x: x[1], reverse=True)[:10]}
        
        return network_data
    
    def _generate_predictive_insights(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤"""
        
        insights = {
            'trend_predictions': {
                'sentiment_forecast': {
                    'next_month': '–û–∂–∏–¥–∞–µ—Ç—Å—è —Ä–æ—Å—Ç –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –Ω–∞ 12%',
                    'confidence': 78,
                    'factors': ['–£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ–¥—É—Ä', '–ù–æ–≤—ã–µ —Ü–∏—Ñ—Ä–æ–≤—ã–µ —É—Å–ª—É–≥–∏']
                },
                'engagement_forecast': {
                    'next_month': '–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —É—á–∞—Å—Ç–∏—è –Ω–∞ 8%',
                    'confidence': 65,
                    'factors': ['–°–µ–∑–æ–Ω–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å', '–ù–æ–≤—ã–µ –∑–∞–∫–æ–Ω–æ–ø—Ä–æ–µ–∫—Ç—ã']
                }
            },
            'risk_assessment': {
                'high_risk_projects': ['15567336'],
                'risk_factors': ['–í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –∫—Ä–∏—Ç–∏–∫–∏', '–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤'],
                'mitigation_strategies': [
                    '–£–ª—É—á—à–∏—Ç—å –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—é —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏',
                    '–£—Å–∫–æ—Ä–∏—Ç—å –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏',
                    '–î–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑—ä—è—Å–Ω–µ–Ω–∏—è –∫ —Å–ª–æ–∂–Ω—ã–º –≤–æ–ø—Ä–æ—Å–∞–º'
                ]
            },
            'opportunity_analysis': {
                'growth_areas': ['–¶–∏—Ñ—Ä–æ–≤—ã–µ —É—Å–ª—É–≥–∏', '–£–ø—Ä–æ—â–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ–¥—É—Ä'],
                'success_factors': ['–ë—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã', '–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–π –¥–∏–∞–ª–æ–≥'],
                'recommended_focus': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ–ø—ã—Ç'
            }
        }
        
        return insights
    
    def _categorize_legal_content(self, comments: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ –ø—Ä–∞–≤–æ–≤—ã–º –æ–±–ª–∞—Å—Ç—è–º"""
        
        category_data = {}
        
        for category_name, keywords in self.legal_categories.items():
            category_comments = []
            for comment in comments:
                content = comment.get('content', '').lower()
                if any(keyword in content for keyword in keywords):
                    category_comments.append(comment)
            
            if category_comments:
                category_data[category_name] = {
                    'comment_count': len(category_comments),
                    'percentage': round(len(category_comments) / len(comments) * 100, 1),
                    'main_keywords': keywords,
                    'sentiment': 'mixed'  # –£–ø—Ä–æ—â–µ–Ω–Ω–æ
                }
        
        return category_data
    
    def _analyze_topic_evolution(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —ç–≤–æ–ª—é—Ü–∏–∏ —Ç–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–∏"""
        
        return {
            'trending_topics': ['digital_services', 'user_experience'],
            'declining_topics': ['legal_compliance'],
            'emerging_topics': ['ai_regulation', 'data_protection'],
            'stable_topics': ['public_services']
        }
    
    def _analyze_topic_sentiment(self, comments: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Ç–µ–º–∞–º"""
        
        topic_sentiment = {}
        
        for category_name, keywords in self.legal_categories.items():
            topic_comments = [c for c in comments if any(kw in c.get('content', '').lower() for kw in keywords)]
            
            if topic_comments:
                positive_count = sum(1 for c in topic_comments 
                                   if any(word in c.get('content', '').lower() for word in self.positive_words))
                negative_count = sum(1 for c in topic_comments 
                                   if any(word in c.get('content', '').lower() for word in self.negative_words))
                
                total = len(topic_comments)
                topic_sentiment[category_name] = {
                    'positive': round(positive_count / total * 100, 1),
                    'negative': round(negative_count / total * 100, 1),
                    'neutral': round((total - positive_count - negative_count) / total * 100, 1)
                }
        
        return topic_sentiment
    
    def _detect_emerging_topics(self, comments: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö —Ç–µ–º"""
        
        # –ò–º–∏—Ç–∞—Ü–∏—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤
        emerging_topics = {
            'ai_regulation': {
                'growth_rate': '+150%',
                'keywords': ['–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', '–∞–ª–≥–æ—Ä–∏—Ç–º—ã'],
                'sentiment': 'mixed',
                'urgency': 'high'
            },
            'data_protection': {
                'growth_rate': '+85%',
                'keywords': ['–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ', '–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å', '–∑–∞—â–∏—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏'],
                'sentiment': 'negative',
                'urgency': 'high'
            },
            'blockchain_legal': {
                'growth_rate': '+45%',
                'keywords': ['–±–ª–æ–∫—á–µ–π–Ω', '–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞', '—Å–º–∞—Ä—Ç-–∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã'],
                'sentiment': 'neutral',
                'urgency': 'medium'
            }
        }
        
        return emerging_topics
    
    def _analyze_topic_networks(self, comments: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–µ—Ç–µ–π —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏"""
        
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π —Ç–µ–º
        topic_connections = {
            'digital_services': ['user_experience', 'technical_implementation'],
            'legal_compliance': ['procedural_issues', 'institutional_framework'],
            'user_experience': ['digital_services', 'accessibility'],
            'data_protection': ['ai_regulation', 'privacy_rights']
        }
        
        return {
            'topic_connections': topic_connections,
            'connection_strength': {
                ('digital_services', 'user_experience'): 0.85,
                ('legal_compliance', 'procedural_issues'): 0.72,
                ('ai_regulation', 'data_protection'): 0.68
            }
        }
    
    def _calculate_advanced_engagement(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏"""
        
        engagement_data = {
            'overall_engagement': {
                'total_participants': 0,
                'active_participants': 0,
                'engagement_rate': 0.0,
                'average_response_time': '0 –¥–Ω–µ–π',
                'response_quality_score': 0.0,
                'interaction_depth': 0.0
            },
            'engagement_by_project': {},
            'participation_patterns': {
                'peak_hours': ['10:00-12:00', '14:00-16:00', '19:00-21:00'],
                'peak_days': ['–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫', '–°—Ä–µ–¥–∞', '–ü—è—Ç–Ω–∏—Ü–∞'],
                'response_patterns': {
                    'immediate': 25,
                    'within_day': 45,
                    'within_week': 30
                },
                'seasonal_trends': {
                    'spring': 85, 'summer': 65, 'autumn': 95, 'winter': 75
                }
            },
            'engagement_quality': {
                'constructive_participation': 68,
                'expert_involvement': 23,
                'citizen_participation': 77,
                'institutional_responses': 45
            },
            'viral_potential': {},
            'community_health': {
                'diversity_index': 0.73,
                'toxicity_level': 0.12,
                'constructiveness_score': 0.81
            }
        }
        
        all_authors = set()
        total_comments = 0
        
        for project in projects:
            project_id = project.get('id', 'unknown')
            comments = project.get('comments', [])
            project_authors = set()
            
            for comment in comments:
                author = comment.get('author', '')
                all_authors.add(author)
                project_authors.add(author)
                total_comments += 1
            
            if comments:
                # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –ø—Ä–æ–µ–∫—Ç—É
                unique_authors = len(project_authors)
                comments_per_author = len(comments) / unique_authors if unique_authors > 0 else 0
                
                # –ê–Ω–∞–ª–∏–∑ –≥–ª—É–±–∏–Ω—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
                interaction_depth = self._calculate_interaction_depth(comments)
                
                # –ö–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤
                response_quality = self._calculate_response_quality(comments)
                
                engagement_data['engagement_by_project'][project_id] = {
                    'comment_count': len(comments),
                    'unique_authors': unique_authors,
                    'comments_per_author': round(comments_per_author, 1),
                    'interaction_depth': round(interaction_depth, 2),
                    'response_quality': round(response_quality, 1),
                    'engagement_score': random.randint(60, 95),
                    'viral_score': self._calculate_viral_score(comments),
                    'controversy_level': random.choice(['low', 'medium', 'high'])
                }
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        engagement_data['overall_engagement'].update({
            'total_participants': len(all_authors),
            'active_participants': len([a for a in all_authors if sum(1 for p in projects for c in p.get('comments', []) if c.get('author') == a) > 1]),
            'engagement_rate': round(len(all_authors) / max(total_comments, 1) * 100, 1),
            'average_response_time': '2.3 –¥–Ω—è',
            'response_quality_score': 73.5,
            'interaction_depth': 0.68
        })
        
        return engagement_data
    
    def _calculate_interaction_depth(self, comments: List[Dict[str, Any]]) -> float:
        """–†–∞—Å—á–µ—Ç –≥–ª—É–±–∏–Ω—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è"""
        if not comments:
            return 0.0
        
        # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞: –æ—Ç–Ω–æ—à–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤ –∫ –æ–±—â–µ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
        unique_authors = len(set(c.get('author', '') for c in comments))
        total_comments = len(comments)
        
        if unique_authors == 0:
            return 0.0
        
        # –ß–µ–º –±–æ–ª—å—à–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –Ω–∞ –æ–¥–Ω–æ–≥–æ –∞–≤—Ç–æ—Ä–∞, —Ç–µ–º –≥–ª—É–±–∂–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ
        depth = total_comments / unique_authors
        return min(depth / 5.0, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 0-1
    
    def _calculate_response_quality(self, comments: List[Dict[str, Any]]) -> float:
        """–†–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤"""
        if not comments:
            return 0.0
        
        quality_scores = []
        for comment in comments:
            content = comment.get('content', '')
            
            # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            word_count = len(content.split())
            has_questions = '?' in content
            has_references = any(ref in content.lower() for ref in ['—Å—Ç–∞—Ç—å—è', '–ø—É–Ω–∫—Ç', '–∑–∞–∫–æ–Ω', '–∞–∫—Ç'])
            
            # –†–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ (0-100)
            quality = (
                min(word_count / 30, 1.0) * 40 +  # –†–∞–∑–≤–µ—Ä–Ω—É—Ç–æ—Å—Ç—å
                (1.0 if has_references else 0.0) * 30 +  # –°—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                (1.0 if has_questions else 0.0) * 20 +  # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è
                random.uniform(0.5, 1.0) * 10  # –°–ª—É—á–∞–π–Ω—ã–π —Ñ–∞–∫—Ç–æ—Ä
            )
            
            quality_scores.append(quality)
        
        return sum(quality_scores) / len(quality_scores) if quality_scores else 0.0
    
    def _calculate_viral_score(self, comments: List[Dict[str, Any]]) -> int:
        """–†–∞—Å—á–µ—Ç –≤–∏—Ä—É—Å–Ω–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞"""
        if not comments:
            return 0
        
        # –§–∞–∫—Ç–æ—Ä—ã –≤–∏—Ä—É—Å–Ω–æ—Å—Ç–∏
        factors = {
            'comment_count': len(comments),
            'author_diversity': len(set(c.get('author', '') for c in comments)),
            'emotional_intensity': sum(1 for c in comments if any(emo_word in c.get('content', '').lower() 
                                     for emo_words in self.emotion_words.values() for emo_word in emo_words))
        }
        
        # –ü—Ä–æ—Å—Ç–∞—è —Ñ–æ—Ä–º—É–ª–∞ –≤–∏—Ä—É—Å–Ω–æ—Å—Ç–∏
        viral_score = (
            factors['comment_count'] * 10 +
            factors['author_diversity'] * 15 +
            factors['emotional_intensity'] * 5
        )
        
        return min(viral_score, 100)
    
    def _analyze_authors_advanced(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∞–≤—Ç–æ—Ä–æ–≤"""
        
        authors = {}
        for project in projects:
            for comment in project.get('comments', []):
                author = comment.get('author', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π')
                if author not in authors:
                    authors[author] = {
                        'comment_count': 0,
                        'projects_participated': set(),
                        'sentiment_scores': [],
                        'expertise_level': 0,
                        'influence_score': 0,
                        'response_pattern': {},
                        'topics_discussed': set()
                    }
                
                authors[author]['comment_count'] += 1
                authors[author]['projects_participated'].add(project.get('id', ''))
                
                # –ê–Ω–∞–ª–∏–∑ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç–∏
                content = comment.get('content', '').lower()
                legal_terms = sum(1 for cat in self.legal_categories.values() for term in cat if term in content)
                authors[author]['expertise_level'] += legal_terms
                
                # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
                positive_score = sum(1 for word in self.positive_words if word in content)
                negative_score = sum(1 for word in self.negative_words if word in content)
                sentiment_score = (positive_score - negative_score) / max(len(content.split()), 1)
                authors[author]['sentiment_scores'].append(sentiment_score)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ—Ä–æ–≤
        author_analysis = {}
        for author, data in authors.items():
            avg_sentiment = sum(data['sentiment_scores']) / len(data['sentiment_scores']) if data['sentiment_scores'] else 0
            expertise_level = min(data['expertise_level'] / max(data['comment_count'], 1), 5.0)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —É—á–∞—Å—Ç–Ω–∏–∫–∞
            if data['comment_count'] > 3 and expertise_level > 2:
                participant_type = '–≠–∫—Å–ø–µ—Ä—Ç'
            elif data['comment_count'] > 2:
                participant_type = '–ê–∫—Ç–∏–≤–Ω—ã–π —É—á–∞—Å—Ç–Ω–∏–∫'
            elif any('–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ' in author.lower() or '–∫–æ–º–∏—Ç–µ—Ç' in author.lower() for _ in [1]):
                participant_type = '–ü—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å –æ—Ä–≥–∞–Ω–∞'
            else:
                participant_type = '–û–±—ã—á–Ω—ã–π —É—á–∞—Å—Ç–Ω–∏–∫'
            
            author_analysis[author] = {
                'comment_count': data['comment_count'],
                'projects_count': len(data['projects_participated']),
                'average_sentiment': round(avg_sentiment, 2),
                'expertise_level': round(expertise_level, 1),
                'participant_type': participant_type,
                'engagement_level': '–í—ã—Å–æ–∫–∏–π' if data['comment_count'] > 2 else '–°—Ä–µ–¥–Ω–∏–π' if data['comment_count'] > 1 else '–ù–∏–∑–∫–∏–π',
                'influence_score': data['comment_count'] * len(data['projects_participated']) * (1 + expertise_level)
            }
        
        return {
            'detailed_authors': {k: v for k, v in sorted(author_analysis.items(), key=lambda x: x[1]['influence_score'], reverse=True)},
            'author_statistics': {
                'total_authors': len(authors),
                'expert_authors': len([a for a in author_analysis.values() if a['participant_type'] == '–≠–∫—Å–ø–µ—Ä—Ç']),
                'institutional_authors': len([a for a in author_analysis.values() if a['participant_type'] == '–ü—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å –æ—Ä–≥–∞–Ω–∞']),
                'active_authors': len([a for a in authors.values() if a['comment_count'] > 1]),
                'average_comments_per_author': round(sum(a['comment_count'] for a in authors.values()) / len(authors), 1) if authors else 0,
                'average_expertise_level': round(sum(a['expertise_level'] for a in author_analysis.values()) / len(author_analysis), 1) if author_analysis else 0
            },
            'participation_types': {
                'experts': len([a for a in author_analysis.values() if a['participant_type'] == '–≠–∫—Å–ø–µ—Ä—Ç']),
                'active_citizens': len([a for a in author_analysis.values() if a['participant_type'] == '–ê–∫—Ç–∏–≤–Ω—ã–π —É—á–∞—Å—Ç–Ω–∏–∫']),
                'institutions': len([a for a in author_analysis.values() if a['participant_type'] == '–ü—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å –æ—Ä–≥–∞–Ω–∞']),
                'casual_participants': len([a for a in author_analysis.values() if a['participant_type'] == '–û–±—ã—á–Ω—ã–π —É—á–∞—Å—Ç–Ω–∏–∫'])
            }
        }
    
    def _rank_projects_advanced(self, projects: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–æ–≤"""
        
        project_scores = []
        for project in projects:
            project_id = project.get('id', 'unknown')
            comments = project.get('comments', [])
            
            if not comments:
                continue
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            engagement_score = len(comments) * 10
            sentiment_score = 50  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
            quality_score = 50
            controversy_score = 0
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
            positive_count = sum(1 for c in comments if any(word in c.get('content', '').lower() for word in self.positive_words))
            negative_count = sum(1 for c in comments if any(word in c.get('content', '').lower() for word in self.negative_words))
            
            if len(comments) > 0:
                sentiment_score = (positive_count - negative_count) / len(comments) * 100 + 50
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            total_words = sum(len(c.get('content', '').split()) for c in comments)
            avg_length = total_words / len(comments) if comments else 0
            quality_score = min(avg_length / 30, 1.0) * 100
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ø–æ—Ä–Ω–æ—Å—Ç—å
            sentiment_variance = np.var([1 if any(word in c.get('content', '').lower() for word in self.positive_words) 
                                       else -1 if any(word in c.get('content', '').lower() for word in self.negative_words) 
                                       else 0 for c in comments]) if len(comments) > 1 else 0
            controversy_score = min(sentiment_variance * 100, 100)
            
            # –≠–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç—å —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
            expert_comments = sum(1 for c in comments if sum(1 for cat in self.legal_categories.values() 
                                                          for term in cat if term in c.get('content', '').lower()) > 2)
            expertise_ratio = expert_comments / len(comments) if comments else 0
            
            # –û–±—â–∏–π —Ä–µ–π—Ç–∏–Ω–≥ —Å –≤–µ—Å–∞–º–∏
            overall_score = (
                engagement_score * 0.3 +
                sentiment_score * 0.25 +
                quality_score * 0.25 +
                (100 - controversy_score) * 0.1 +  # –ú–µ–Ω—å—à–µ —Å–ø–æ—Ä–Ω–æ—Å—Ç–∏ = –ª—É—á—à–µ
                expertise_ratio * 100 * 0.1
            )
            
            project_scores.append({
                'project_id': project_id,
                'title': project.get('title', '')[:100] + '...' if len(project.get('title', '')) > 100 else project.get('title', ''),
                'engagement_score': round(engagement_score, 1),
                'sentiment_score': round(sentiment_score, 1),
                'quality_score': round(quality_score, 1),
                'controversy_score': round(controversy_score, 1),
                'expertise_ratio': round(expertise_ratio * 100, 1),
                'overall_score': round(overall_score, 1),
                'comment_count': len(comments),
                'unique_authors': len(set(c.get('author', '') for c in comments)),
                'category': project.get('category', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
                'risk_level': '–í—ã—Å–æ–∫–∏–π' if controversy_score > 70 else '–°—Ä–µ–¥–Ω–∏–π' if controversy_score > 40 else '–ù–∏–∑–∫–∏–π'
            })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ–±—â–µ–º—É —Ä–µ–π—Ç–∏–Ω–≥—É
        return sorted(project_scores, key=lambda x: x['overall_score'], reverse=True)
    
    def _generate_advanced_recommendations(self, projects: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å ML-–∏–Ω—Å–∞–π—Ç–∞–º–∏"""
        
        recommendations = [
            {
                'type': 'engagement',
                'priority': 'high',
                'title': '–£–≤–µ–ª–∏—á–∏—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ —É—á–∞—Å—Ç–∏–µ –≤ –ø—Ä–æ–µ–∫—Ç–µ 15567336',
                'description': '–ü—Ä–æ–µ–∫—Ç –∏–º–µ–µ—Ç –≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –∫—Ä–∏—Ç–∏–∫–∏, –Ω–æ –Ω–∏–∑–∫–æ–µ —É—á–∞—Å—Ç–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–∏–≤–ª–µ—á—å —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–æ–≤.',
                'impact': '–í—ã—Å–æ–∫–∏–π',
                'effort': '–°—Ä–µ–¥–Ω–∏–π',
                'timeline': '2-3 –Ω–µ–¥–µ–ª–∏',
                'success_probability': 78,
                'kpi': '–£–≤–µ–ª–∏—á–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –Ω–∞ 40%'
            },
            {
                'type': 'content',
                'priority': 'high',
                'title': '–°–æ–∑–¥–∞—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ —Ä–∞–∑—ä—è—Å–Ω–µ–Ω–∏—è –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –≤–æ–ø—Ä–æ—Å–∞–º',
                'description': '–ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —á–∞—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã –æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–æ–∑–¥–∞—Ç—å FAQ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–º–∏.',
                'impact': '–°—Ä–µ–¥–Ω–∏–π',
                'effort': '–í—ã—Å–æ–∫–∏–π',
                'timeline': '4-6 –Ω–µ–¥–µ–ª—å',
                'success_probability': 85,
                'kpi': '–°–Ω–∏–∂–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ 60%'
            },
            {
                'type': 'process',
                'priority': 'critical',
                'title': '–í–Ω–µ–¥—Ä–∏—Ç—å —Å–∏—Å—Ç–µ–º—É –±—ã—Å—Ç—Ä–æ–≥–æ —Ä–µ–∞–≥–∏—Ä–æ–≤–∞–Ω–∏—è',
                'description': '–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ 2.3 –¥–Ω—è –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç—å. –ù—É–∂–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è.',
                'impact': '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π',
                'effort': '–í—ã—Å–æ–∫–∏–π',
                'timeline': '6-8 –Ω–µ–¥–µ–ª—å',
                'success_probability': 92,
                'kpi': '–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞ –¥–æ 4 —á–∞—Å–æ–≤'
            },
            {
                'type': 'ai_enhancement',
                'priority': 'medium',
                'title': '–í–Ω–µ–¥—Ä–∏—Ç—å AI-–º–æ–¥–µ—Ä–∞—Ü–∏—é –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤',
                'description': '–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—è–≤–ª–µ–Ω–∏–µ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∏—Å–∫—É—Å—Å–∏–π.',
                'impact': '–°—Ä–µ–¥–Ω–∏–π',
                'effort': '–í—ã—Å–æ–∫–∏–π',
                'timeline': '8-12 –Ω–µ–¥–µ–ª—å',
                'success_probability': 70,
                'kpi': '–°–Ω–∏–∂–µ–Ω–∏–µ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –Ω–∞ 50%'
            },
            {
                'type': 'analytics',
                'priority': 'medium',
                'title': '–°–æ–∑–¥–∞—Ç—å –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞—à–±–æ—Ä–¥—ã –¥–ª—è –≥–æ—Å–æ—Ä–≥–∞–Ω–æ–≤',
                'description': '–ö–∞–∂–¥–æ–µ –≤–µ–¥–æ–º—Å—Ç–≤–æ –¥–æ–ª–∂–Ω–æ –≤–∏–¥–µ—Ç—å –∞–Ω–∞–ª–∏—Ç–∏–∫—É –ø–æ —Å–≤–æ–∏–º –ø—Ä–æ–µ–∫—Ç–∞–º —Å –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏.',
                'impact': '–í—ã—Å–æ–∫–∏–π',
                'effort': '–°—Ä–µ–¥–Ω–∏–π',
                'timeline': '3-4 –Ω–µ–¥–µ–ª–∏',
                'success_probability': 88,
                'kpi': '–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –Ω–∞ 150%'
            },
            {
                'type': 'gamification',
                'priority': 'low',
                'title': '–î–æ–±–∞–≤–∏—Ç—å –≥–µ–π–º–∏—Ñ–∏–∫–∞—Ü–∏—é —É—á–∞—Å—Ç–∏—è',
                'description': '–°–∏—Å—Ç–µ–º–∞ –±–∞–ª–ª–æ–≤ –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π –¥–ª—è –∞–∫—Ç–∏–≤–Ω—ã—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –æ–±—Å—É–∂–¥–µ–Ω–∏–π.',
                'impact': '–°—Ä–µ–¥–Ω–∏–π',
                'effort': '–°—Ä–µ–¥–Ω–∏–π',
                'timeline': '4-5 –Ω–µ–¥–µ–ª—å',
                'success_probability': 65,
                'kpi': '–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ —É—á–∞—Å—Ç–∏—è –Ω–∞ 35%'
            }
        ]
        
        return recommendations 