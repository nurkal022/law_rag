import os
import re
import numpy as np
from typing import List, Dict, Tuple
from sentence_transformers import SentenceTransformer
import tiktoken
from config import Config
from database.models import DatabaseManager

class DocumentProcessor:
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.embedding_model = None
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å
        try:
            self.embedding_model = SentenceTransformer(Config.EMBEDDING_MODEL)
            print(f"‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {Config.EMBEDDING_MODEL}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å
            try:
                print("üîÑ –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å...")
                self.embedding_model = SentenceTransformer(Config.EMBEDDING_MODEL_OFFLINE)
                print(f"‚úÖ –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {Config.EMBEDDING_MODEL_OFFLINE}")
            except Exception as e2:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏: {e2}")
                print("‚ö†Ô∏è  –°–∏—Å—Ç–µ–º–∞ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
                self.embedding_model = None
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        
    def clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        text = re.sub(r'\s+', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        text = re.sub(r'[^\w\s\.\,\;\:\!\?\-\(\)\[\]\"\'‚Ññ¬ß]', '', text)
        
        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–æ—á–∫–∏
        text = re.sub(r'\.{3,}', '...', text)
        
        return text.strip()
    
    def extract_title_from_filename(self, filename: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Ç–∞–µ–º–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
        # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        title = os.path.splitext(filename)[0]
        
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω "–Ω–∞–∑–≤–∞–Ω–∏–µ –ù–æ–≤—ã–π –æ–ø–∏—Å–∞–Ω–∏–µ"
        if " –ù–æ–≤—ã–π " in title:
            parts = title.split(" –ù–æ–≤—ã–π ")
            return parts[0].strip()
        
        return title
    
    def split_into_chunks(self, text: str, chunk_size: int = None, overlap: int = None) -> List[Dict]:
        """–†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ —Å —É—á–µ—Ç–æ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≥—Ä–∞–Ω–∏—Ü"""
        chunk_size = chunk_size or Config.CHUNK_SIZE
        overlap = overlap or Config.CHUNK_OVERLAP
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
        cleaned_text = self.clean_text(text)
        
        chunks = []
        start = 0
        chunk_index = 0
        
        while start < len(cleaned_text):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω–µ—Ü —á–∞–Ω–∫–∞
            end = min(start + chunk_size, len(cleaned_text))
            
            # –ï—Å–ª–∏ –Ω–µ –¥–æ—Å—Ç–∏–≥–ª–∏ –∫–æ–Ω—Ü–∞ —Ç–µ–∫—Å—Ç–∞, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –±–ª–∏–∂–∞–π—à—É—é –≥—Ä–∞–Ω–∏—Ü—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if end < len(cleaned_text):
                # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö overlap
                sentence_end = cleaned_text.rfind('.', end - overlap, end)
                if sentence_end != -1 and sentence_end > start:
                    end = sentence_end + 1
                else:
                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–∫—É, –∏—â–µ–º –ø—Ä–æ–±–µ–ª
                    space_pos = cleaned_text.rfind(' ', end - overlap, end)
                    if space_pos != -1 and space_pos > start:
                        end = space_pos
            
            chunk_content = cleaned_text[start:end].strip()
            
            if chunk_content:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —á–∞–Ω–∫–∏
                chunks.append({
                    'index': chunk_index,
                    'content': chunk_content,
                    'start_position': start,
                    'end_position': end,
                    'size': len(chunk_content)
                })
                chunk_index += 1
            
            # –°–ª–µ–¥—É—é—â–∏–π —á–∞–Ω–∫ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —É—á–µ—Ç–æ–º overlap
            start = max(end - overlap, start + 1)
            
            # –ó–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
            if start >= end:
                break
        
        return chunks
    
    def create_embeddings(self, texts: List[str]) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ embeddings –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        if not texts:
            return np.array([])
        
        if not self.embedding_model:
            print("‚ö†Ô∏è  –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ embeddings")
            return np.array([])
        
        try:
            embeddings = self.embedding_model.encode(texts, convert_to_tensor=False, show_progress_bar=True)
            return np.array(embeddings)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ embeddings: {e}")
            return np.array([])
    
    def process_document(self, filepath: str) -> bool:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞: –∑–∞–≥—Ä—É–∑–∫–∞, —Ä–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏, —Å–æ–∑–¥–∞–Ω–∏–µ embeddings"""
        try:
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
            with open(filepath, 'r', encoding='utf-8') as file:
                content = file.read()
            
            if not content.strip():
                print(f"–§–∞–π–ª {filepath} –ø—É—Å—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                return False
            
            filename = os.path.basename(filepath)
            title = self.extract_title_from_filename(filename)
            
            print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç: {title}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –±–∞–∑—É
            document_id = self.db_manager.insert_document(filename, content, title)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self.split_into_chunks(content)
            
            if not chunks:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏ –¥–ª—è {filename}")
                return False
            
            print(f"–°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            # –°–æ–∑–¥–∞–µ–º embeddings –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ —Å—Ä–∞–∑—É
            chunk_contents = [chunk['content'] for chunk in chunks]
            embeddings = self.create_embeddings(chunk_contents)
            
            if len(embeddings) == 0 and self.embedding_model:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å embeddings –¥–ª—è {filename}")
                return False
            elif len(embeddings) == 0:
                print(f"‚ö†Ô∏è  –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç {filename} –±–µ–∑ embeddings")
                embeddings = [None] * len(chunks)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ —Å embeddings –≤ –±–∞–∑—É
            for i, chunk in enumerate(chunks):
                embedding = embeddings[i] if i < len(embeddings) else None
                self.db_manager.insert_chunk(
                    document_id=document_id,
                    chunk_index=chunk['index'],
                    content=chunk['content'],
                    start_pos=chunk['start_position'],
                    end_pos=chunk['end_position'],
                    embedding=embedding
                )
            
            print(f"–î–æ–∫—É–º–µ–Ω—Ç {title} —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
            return True
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {filepath}: {e}")
            return False
    
    def process_all_documents(self, documents_dir: str) -> Dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        if not os.path.exists(documents_dir):
            print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {documents_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø—É—Ç–∏
            alternatives = [
                os.path.join('current', 'examples'),
                'current/examples',
                'examples',
                os.path.join(os.path.dirname(os.path.dirname(__file__)), 'current', 'examples')
            ]
            
            for alt_path in alternatives:
                if os.path.exists(alt_path):
                    print(f"üìÅ –ù–∞–π–¥–µ–Ω–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {alt_path}")
                    documents_dir = alt_path
                    break
            else:
                return {
                    'processed': 0,
                    'failed': 0,
                    'total': 0,
                    'errors': [f'–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {documents_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø—É—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã']
                }
        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã
        unprocessed_docs = self.db_manager.get_unprocessed_documents()
        
        if not unprocessed_docs:
            print("–í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏–ª–∏ –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç–∞—è")
            # –ï—Å–ª–∏ –±–∞–∑–∞ –ø—É—Å—Ç–∞—è, –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
            stats = self.db_manager.get_documents_stats()
            if stats['documents_count'] == 0:
                print("–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
                load_result = self.db_manager.bulk_load_documents_from_directory(documents_dir)
                unprocessed_docs = self.db_manager.get_unprocessed_documents()
                
                if not unprocessed_docs:
                    return {'processed': 0, 'failed': 0, 'errors': ['–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã']}
        
        print(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(unprocessed_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        processed = 0
        failed = 0
        errors = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–∞–∫–µ—Ç–∞–º–∏
        batch_size = 10
        for i in range(0, len(unprocessed_docs), batch_size):
            batch = unprocessed_docs[i:i + batch_size]
            
            print(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞–∫–µ—Ç {i//batch_size + 1}/{(len(unprocessed_docs) + batch_size - 1)//batch_size}")
            
            try:
                if self._process_documents_batch(batch):
                    processed += len(batch)
                    print(f"‚úÖ –ü–∞–∫–µ—Ç {i//batch_size + 1} –æ–±—Ä–∞–±–æ—Ç–∞–Ω —É—Å–ø–µ—à–Ω–æ")
                else:
                    failed += len(batch)
                    errors.append(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞–∫–µ—Ç–∞ {i//batch_size + 1}")
            except Exception as e:
                failed += len(batch)
                error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –ø–∞–∫–µ—Ç–µ {i//batch_size + 1}: {str(e)}"
                errors.append(error_msg)
                print(f"‚ùå {error_msg}")
        
        result = {
            'processed': processed,
            'failed': failed,
            'total': len(unprocessed_docs),
            'errors': errors
        }
        
        print(f"\nüìä –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {processed}")
        print(f"‚ùå –û—à–∏–±–æ–∫: {failed}")
        print(f"üìÑ –í—Å–µ–≥–æ: {len(unprocessed_docs)}")
        
        return result
    
    def _process_documents_batch(self, documents: List[Dict]) -> bool:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            all_chunks = []
            
            for doc in documents:
                print(f"  üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {doc['title']}")
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
                chunks = self.split_into_chunks(doc['content'])
                
                if not chunks:
                    print(f"    ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏")
                    continue
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è –º–∞—Å—Å–æ–≤–æ–π –≤—Å—Ç–∞–≤–∫–∏
                for chunk in chunks:
                    chunk_data = {
                        'document_id': doc['id'],
                        'chunk_index': chunk['index'],
                        'content': chunk['content'],
                        'start_position': chunk['start_position'],
                        'end_position': chunk['end_position'],
                        'chunk_size': chunk['size'],
                        'embedding': None  # Embeddings —Å–æ–∑–¥–∞–¥–∏–º –æ—Ç–¥–µ–ª—å–Ω–æ
                    }
                    all_chunks.append(chunk_data)
                
                print(f"    ‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            if all_chunks:
                # –ú–∞—Å—Å–æ–≤–∞—è –≤—Å—Ç–∞–≤–∫–∞ —á–∞–Ω–∫–æ–≤
                self.db_manager.bulk_insert_chunks(all_chunks)
                
                # –°–æ–∑–¥–∞–µ–º embeddings –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ –ø–∞–∫–µ—Ç–∞
                return self._create_embeddings_for_batch(all_chunks)
            
            return True
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
            return False
    
    def _create_embeddings_for_batch(self, chunks_data: List[Dict]) -> bool:
        """–°–æ–∑–¥–∞–Ω–∏–µ embeddings –¥–ª—è –ø–∞–∫–µ—Ç–∞ —á–∞–Ω–∫–æ–≤"""
        try:
            print(f"  üß† –°–æ–∑–¥–∞–µ–º embeddings –¥–ª—è {len(chunks_data)} —á–∞–Ω–∫–æ–≤...")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã
            texts = [chunk['content'] for chunk in chunks_data]
            
            # –°–æ–∑–¥–∞–µ–º embeddings
            embeddings = self.create_embeddings(texts)
            
            if len(embeddings) == 0 and self.embedding_model:
                print("  ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å embeddings")
                return False
            elif len(embeddings) == 0:
                print("  ‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ embeddings")
                return True
            
            # –û–±–Ω–æ–≤–ª—è–µ–º embeddings –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ SQLAlchemy
            from database.models import DocumentChunk, db
            
            try:
                for i, chunk in enumerate(chunks_data):
                    if i < len(embeddings):
                        # –ù–∞—Ö–æ–¥–∏–º —á–∞–Ω–∫ –≤ –±–∞–∑–µ
                        db_chunk = DocumentChunk.query.filter_by(
                            document_id=chunk['document_id'],
                            chunk_index=chunk['chunk_index']
                        ).first()
                        
                        if db_chunk:
                            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º embedding
                            db_chunk.set_embedding(embeddings[i])
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
                db.session.commit()
                print(f"  ‚úÖ Embeddings —Å–æ–∑–¥–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
                return True
                
            except Exception as db_error:
                db.session.rollback()
                print(f"  ‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –±–∞–∑—É: {db_error}")
                return False
            
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è embeddings: {e}")
            return False
    
    def update_embeddings(self) -> bool:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ embeddings –¥–ª—è —á–∞–Ω–∫–æ–≤ –±–µ–∑ –Ω–∏—Ö"""
        if not self.embedding_model:
            print("‚ö†Ô∏è  –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ")
            return True
            
        try:
            # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –±–µ–∑ embeddings —á–µ—Ä–µ–∑ SQLAlchemy
            from database.models import DocumentChunk, db
            
            chunks_without_embeddings = DocumentChunk.query.filter(
                DocumentChunk.embedding.is_(None)
            ).all()
            
            if not chunks_without_embeddings:
                print("–í—Å–µ —á–∞–Ω–∫–∏ —É–∂–µ –∏–º–µ—é—Ç embeddings")
                return True
            
            print(f"–°–æ–∑–¥–∞–µ–º embeddings –¥–ª—è {len(chunks_without_embeddings)} —á–∞–Ω–∫–æ–≤")
            
            # –°–æ–∑–¥–∞–µ–º embeddings
            contents = [chunk.content for chunk in chunks_without_embeddings]
            embeddings = self.create_embeddings(contents)
            
            if len(embeddings) == 0:
                print("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å embeddings")
                return False
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            try:
                for i, chunk in enumerate(chunks_without_embeddings):
                    if i < len(embeddings):
                        chunk.set_embedding(embeddings[i])
                
                db.session.commit()
                print("Embeddings —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω—ã")
                return True
                
            except Exception as db_error:
                db.session.rollback()
                print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è embeddings: {db_error}")
                return False
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ embeddings: {e}")
            return False 